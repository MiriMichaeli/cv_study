{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction model for workout types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import splitfolders \n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = '/Users/mirimichaeli/projects/video_classisifcation/workoutfitness-video'\n",
    "folders_to_keep = ['deadlift', 'push-up', 'russian twist'] # TODO delete, this is only for simplifying the model + further use of the selected folders!\n",
    "destination_dir = './dataset'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "for folder in folders_to_keep:\n",
    "    source = os.path.join(data_dir, folder)\n",
    "    destination = os.path.join(destination_dir, folder.replace(\" \",\"_\"))\n",
    "    \n",
    "    # Check if the destination folder already exists\n",
    "    if os.path.exists(destination):\n",
    "        print(f\"Destination folder '{destination}' already exists. Deleting existing folder.\")\n",
    "        shutil.rmtree(destination)  # Delete the existing folder\n",
    "    \n",
    "    # Copy the source folder to the destination\n",
    "    shutil.copytree(source, destination)\n",
    "    print(f\"Folder '{folder}' copied to '{destination}'.\")\n",
    "    print()\n",
    "\n",
    "selected_folders = os.listdir(destination_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data into train, test, validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the videos:\n",
    "output_folder = \"./partitioned_videos_for_model\"\n",
    "splitfolders.ratio(destination_dir, output=output_folder, seed=1337, ratio=(.7, 0.15,0.15))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the train directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the directories for train, test, and validation\n",
    "train_dir = f'{output_folder}/train'\n",
    "test_dir = f'{output_folder}/test'\n",
    "validation_dir = f'{output_folder}/val'\n",
    "\n",
    "# Set the dimensions for resizing the frames\n",
    "target_dims = (224, 224)\n",
    "\n",
    "# Initialize the data and labels lists\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load and preprocess the videos from the training set\n",
    "for category in os.listdir(train_dir):\n",
    "    category_dir = os.path.join(train_dir, category)\n",
    "    for video_file in os.listdir(category_dir):\n",
    "        video_path = os.path.join(category_dir, video_file)\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break \n",
    "            frame = cv2.resize(frame, target_dims)\n",
    "            frame = img_to_array(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        if len(frames) > 0:\n",
    "            data.extend(frames)\n",
    "            labels.extend([category] * len(frames))  # Add label for each video\n",
    "\n",
    "# Convert the labels to binary array representation\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform([[label] for label in labels])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained InceptionV3 model with local weights file\n",
    "weights_path = './inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "base_model = InceptionV3(include_top=False, weights=weights_path, input_shape=(target_dims[0], target_dims[1], 3))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(mlb.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data and labels to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Train the model\n",
    "epochs_number = 3 # was 10, I saw that the model reached a plateau after 3-5 runs.\n",
    "history = model.fit(data, labels, epochs=epochs_number, batch_size=16)\n",
    "# TODO use history variable for learning curve etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the frames from the testing set\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for category in os.listdir(test_dir):\n",
    "    category_dir = os.path.join(test_dir, category)\n",
    "    try:\n",
    "        for video_file in os.listdir(category_dir):\n",
    "            video_path = os.path.join(category_dir, video_file)\n",
    "            frames = []\n",
    "            test_frames_label = []\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, target_dims)\n",
    "                frame = img_to_array(frame)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            if len(frames) > 0:\n",
    "                test_data.extend(frames)\n",
    "                test_labels.extend([category] * len(frames))  # Add label for each video\n",
    "    except NotADirectoryError as nde:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform one-hot encoding on the test labels\n",
    "test_labels = mlb.fit_transform([label] for label in test_labels)\n",
    "\n",
    "# Convert the data and labels to numpy arrays\n",
    "# test_data = np.array(test_data)\n",
    "# test_labels = np.array(test_labels)\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "predictions = []\n",
    "for video_frames in test_data:\n",
    "    video_frames = np.array(video_frames)\n",
    "    video_frames = np.expand_dims(video_frames, axis=0)\n",
    "    pred = model.predict(video_frames)\n",
    "    predictions.append(pred)\n",
    "preds = np.concatenate(predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "report = classification_report(np.argmax(test_labels, axis=1), pred_labels)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert test_labels and pred_labels to 1D arrays if needed\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(test_labels, pred_labels)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the valildation set:\n",
    "val_data = []\n",
    "val_labels = []\n",
    "for category in os.listdir(validation_dir):\n",
    "    category_dir = os.path.join(validation_dir, category)\n",
    "    try:\n",
    "        for video_file in os.listdir(category_dir):\n",
    "            video_path = os.path.join(category_dir, video_file)\n",
    "            frames = []\n",
    "            test_frames_label = []\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, target_dims)\n",
    "                frame = img_to_array(frame)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            if len(frames) > 0:\n",
    "                val_data.extend(frames)\n",
    "                val_labels.extend([category] * len(frames))  # Add label for each video\n",
    "    except NotADirectoryError as nde:\n",
    "        continue\n",
    "\n",
    "# Perform one-hot encoding on the test labels\n",
    "val_labels = mlb.fit_transform([label] for label in val_labels)\n",
    "\n",
    "# Convert validation data and labels to NumPy arrays\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(val_data, val_labels)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the predicted probabilities for each class\n",
    "predictions = model.predict(val_data)\n",
    "\n",
    "# Aggregate frame-level predictions to obtain video-level predictions\n",
    "video_predictions = []\n",
    "video_labels = set(folders_to_keep)  # Set of unique labels in the validation set - TODO collect dir names\n",
    "\n",
    "# Iterate over each video in the validation set\n",
    "for category in os.listdir(validation_dir):\n",
    "    category_dir = os.path.join(validation_dir, category)\n",
    "    try:\n",
    "        for video_file in os.listdir(category_dir):\n",
    "            video_frames = []  # Frames for the current video\n",
    "            video_label = None  # True label for the current video\n",
    "        \n",
    "            # Collect frames and true label for the current video\n",
    "            for i, file in enumerate(val_data):\n",
    "                if video_file in file:\n",
    "                    video_frames.append(predictions[i])\n",
    "                    video_label = val_labels[i]\n",
    "        \n",
    "            # Perform majority voting or averaging on the frame-level predictions\n",
    "            video_prediction = np.mean(video_frames, axis=0)  # Use mean for averaging\n",
    "            \n",
    "            # Assign the predicted label based on the highest probability\n",
    "            predicted_label = np.argmax(video_prediction)\n",
    "            \n",
    "            # Append the predicted label to the list of video predictions\n",
    "            video_predictions.append(predicted_label)\n",
    "\n",
    "    except NotADirectoryError as nde:\n",
    "        continue\n",
    "     \n",
    "# Print the predicted labels for each video\n",
    "for i, video_file in enumerate(os.listdir(validation_dir)):\n",
    "    print(f\"Video: {video_file} - Predicted Label: {video_predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things TODO:\n",
    "\n",
    "### check the videos size to see if the 224,224 is a good choice\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def get_frame_size(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    return frame.shape[:2]  # Return only the height and width\n",
    "\n",
    "for category in os.listdir(train_dir):\n",
    "    category_dir = os.path.join(train_dir, category)\n",
    "    for video_file in os.listdir(category_dir):\n",
    "        video_path = os.path.join(category_dir, video_file)\n",
    "        frame_size = get_frame_size(video_path)\n",
    "        print(f\"Video: {video_file}, Frame Size: {frame_size}\")\n",
    "\n",
    "\n",
    "### change to multi-class instead of multi-label\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Assuming you have a list of class labels\n",
    "labels = ['label1', 'label2', 'label3', ...]\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "# One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_labels = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
